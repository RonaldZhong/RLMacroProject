{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd68cd72",
   "metadata": {},
   "source": [
    "# Gym Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e802101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Shape: (4,)\n",
      "Action space: Discrete(2)\n",
      "Sampled action: 0\n",
      "(4,) 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Box(4,) means that it is a Vector with 4 components\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Shape:\", env.observation_space.shape)\n",
    "# Discrete(2) means that there is two discrete actions\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# The reset method is called at the beginning of an episode\n",
    "obs = env.reset()\n",
    "# Sample a random action\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "obs, reward, done, info = env.step(action)\n",
    "# Note the obs is a numpy array\n",
    "# info is an empty dict for now but can contain any debugging info\n",
    "# reward is a scalar\n",
    "print(obs.shape, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d759ce",
   "metadata": {},
   "source": [
    "# A GoLeft Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604a9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left. \n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Define constants for clearer code\n",
    "  LEFT = 0\n",
    "  RIGHT = 1\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions, we have two: left and right\n",
    "    n_actions = 2\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = self.grid_size - 1\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    if action == self.LEFT:\n",
    "      self.agent_pos -= 1\n",
    "    elif action == self.RIGHT:\n",
    "      self.agent_pos += 1\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    done = bool(self.agent_pos == 0)\n",
    "\n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    print(\".\" * self.agent_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c497e40",
   "metadata": {},
   "source": [
    "## Validate the Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4b8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb82778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aec712",
   "metadata": {},
   "source": [
    "## Test the Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d7a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........x.\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Discrete(2)\n",
      "1\n",
      "Step 1\n",
      "obs= [8.] reward= 0 done= False\n",
      "........x..\n",
      "Step 2\n",
      "obs= [7.] reward= 0 done= False\n",
      ".......x...\n",
      "Step 3\n",
      "obs= [6.] reward= 0 done= False\n",
      "......x....\n",
      "Step 4\n",
      "obs= [5.] reward= 0 done= False\n",
      ".....x.....\n",
      "Step 5\n",
      "obs= [4.] reward= 0 done= False\n",
      "....x......\n",
      "Step 6\n",
      "obs= [3.] reward= 0 done= False\n",
      "...x.......\n",
      "Step 7\n",
      "obs= [2.] reward= 0 done= False\n",
      "..x........\n",
      "Step 8\n",
      "obs= [1.] reward= 0 done= False\n",
      ".x.........\n",
      "Step 9\n",
      "obs= [0.] reward= 1 done= True\n",
      "x..........\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  obs, reward, done, info = env.step(GO_LEFT)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render()\n",
    "  if done:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce7b4a",
   "metadata": {},
   "source": [
    "## Solve with stable baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2e419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "env = GoLeftEnv(grid_size=10)\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env, n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f235045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 13.4     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 952      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.355   |\n",
      "|    explained_variance | -28.3    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.0561  |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 11.4     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1055     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.197   |\n",
      "|    explained_variance | -3.94    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00421  |\n",
      "|    value_loss         | 0.00649  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.66      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 1066      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0756   |\n",
      "|    explained_variance | -0.298    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000789 |\n",
      "|    value_loss         | 0.00239   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.4      |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1086     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.058   |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.000348 |\n",
      "|    value_loss         | 0.00026  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.24      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 1079      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0326   |\n",
      "|    explained_variance | -2.95     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -7.84e-05 |\n",
      "|    value_loss         | 0.000774  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.06      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 1083      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0157   |\n",
      "|    explained_variance | 0.832     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -4.56e-05 |\n",
      "|    value_loss         | 0.00035   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.02     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1100     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0196  |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 4.34e-05 |\n",
      "|    value_loss         | 9.91e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.04      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 1104      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00351  |\n",
      "|    explained_variance | 0.953     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -3.02e-06 |\n",
      "|    value_loss         | 6.9e-05   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.02      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 1112      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00975  |\n",
      "|    explained_variance | 0.901     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -2.04e-05 |\n",
      "|    value_loss         | 0.000203  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.02     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1122     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00442 |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.21e-05 |\n",
      "|    value_loss         | 0.000159 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "model = A2C('MlpPolicy', env, verbose=1).learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134f237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Action:  [0]\n",
      "obs= [[8.]] reward= [0.] done= [False]\n",
      "........x..\n",
      "Step 2\n",
      "Action:  [0]\n",
      "obs= [[7.]] reward= [0.] done= [False]\n",
      ".......x...\n",
      "Step 3\n",
      "Action:  [0]\n",
      "obs= [[6.]] reward= [0.] done= [False]\n",
      "......x....\n",
      "Step 4\n",
      "Action:  [0]\n",
      "obs= [[5.]] reward= [0.] done= [False]\n",
      ".....x.....\n",
      "Step 5\n",
      "Action:  [0]\n",
      "obs= [[4.]] reward= [0.] done= [False]\n",
      "....x......\n",
      "Step 6\n",
      "Action:  [0]\n",
      "obs= [[3.]] reward= [0.] done= [False]\n",
      "...x.......\n",
      "Step 7\n",
      "Action:  [0]\n",
      "obs= [[2.]] reward= [0.] done= [False]\n",
      "..x........\n",
      "Step 8\n",
      "Action:  [0]\n",
      "obs= [[1.]] reward= [0.] done= [False]\n",
      ".x.........\n",
      "Step 9\n",
      "Action:  [0]\n",
      "obs= [[9.]] reward= [1.] done= [ True]\n",
      ".........x.\n",
      "Goal reached! reward= [1.]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  action, _ = model.predict(obs, deterministic=True)\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  print(\"Action: \", action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render(mode='console')\n",
    "  if done:\n",
    "    # Note that the VecEnv resets automatically\n",
    "    # when a done signal is encountered\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76922774",
   "metadata": {},
   "source": [
    "# The Continuous Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7090b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class GoLeftEnvContinuous(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left. \n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnvContinuous, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    self.action_space = spaces.Box(low=-1, high=1,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    # self.agent_pos = self.grid_size - 1\n",
    "    # Random Initialize\n",
    "    self.agent_pos = self.observation_space.sample()[0]\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    if self.action_space.contains(action):\n",
    "      self.agent_pos += action[0]\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    done = bool(self.agent_pos == 0)\n",
    "\n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    approx_pos = int(np.ceil(self.agent_pos))\n",
    "    print(\".\" * approx_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - approx_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40fc6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnvContinuous()\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "290f79de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........x.\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Box(-1.0, 1.0, (1,), float32)\n",
      "[-0.2842394]\n",
      "Step 1\n",
      "obs= [7.4067564] reward= 0 done= False\n",
      "........x..\n",
      "Step 2\n",
      "obs= [6.4067564] reward= 0 done= False\n",
      ".......x...\n",
      "Step 3\n",
      "obs= [5.4067564] reward= 0 done= False\n",
      "......x....\n",
      "Step 4\n",
      "obs= [4.4067564] reward= 0 done= False\n",
      ".....x.....\n",
      "Step 5\n",
      "obs= [3.4067564] reward= 0 done= False\n",
      "....x......\n",
      "Step 6\n",
      "obs= [2.4067564] reward= 0 done= False\n",
      "...x.......\n",
      "Step 7\n",
      "obs= [1.4067564] reward= 0 done= False\n",
      "..x........\n",
      "Step 8\n",
      "obs= [0.4067564] reward= 0 done= False\n",
      ".x.........\n",
      "Step 9\n",
      "obs= [0.] reward= 1 done= True\n",
      "x..........\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = np.array([-1])\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  obs, reward, done, info = env.step(GO_LEFT)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render()\n",
    "  if done:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4cc46c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 15.2     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 925      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | -0.613   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.0676  |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 0.00187  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 10.9     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1053     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.77     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.0437   |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 0.00451  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.44     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1042     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.0333   |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 0.00103  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.62     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1062     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | -0.369   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.0871  |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 0.00222  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.39     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1076     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.564    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000588 |\n",
      "|    std                | 0.987    |\n",
      "|    value_loss         | 8.12e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.08     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1067     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.631    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    std                | 0.985    |\n",
      "|    value_loss         | 0.00032  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.91     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1060     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00525 |\n",
      "|    std                | 0.981    |\n",
      "|    value_loss         | 5.6e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.86     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1059     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00813 |\n",
      "|    std                | 0.971    |\n",
      "|    value_loss         | 6.1e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6        |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1059     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.464   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.0374  |\n",
      "|    std                | 0.97     |\n",
      "|    value_loss         | 0.00221  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.55     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 1058     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0159   |\n",
      "|    std                | 0.967    |\n",
      "|    value_loss         | 0.000104 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "env = GoLeftEnvContinuous(grid_size=10)\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "# Train the agent\n",
    "model = A2C('MlpPolicy', env, verbose=1).learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a654e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs= [[8.282103]]\n",
      "Step 1\n",
      "Action:  [[-1.]]\n",
      "obs= [[7.2821026]] reward= [0.] done= [False]\n",
      "........x..\n",
      "Step 2\n",
      "Action:  [[-1.]]\n",
      "obs= [[6.2821026]] reward= [0.] done= [False]\n",
      ".......x...\n",
      "Step 3\n",
      "Action:  [[-1.]]\n",
      "obs= [[5.2821026]] reward= [0.] done= [False]\n",
      "......x....\n",
      "Step 4\n",
      "Action:  [[-1.]]\n",
      "obs= [[4.2821026]] reward= [0.] done= [False]\n",
      ".....x.....\n",
      "Step 5\n",
      "Action:  [[-1.]]\n",
      "obs= [[3.2821026]] reward= [0.] done= [False]\n",
      "....x......\n",
      "Step 6\n",
      "Action:  [[-1.]]\n",
      "obs= [[2.2821026]] reward= [0.] done= [False]\n",
      "...x.......\n",
      "Step 7\n",
      "Action:  [[-1.]]\n",
      "obs= [[1.2821026]] reward= [0.] done= [False]\n",
      "..x........\n",
      "Step 8\n",
      "Action:  [[-1.]]\n",
      "obs= [[0.28210258]] reward= [0.] done= [False]\n",
      ".x.........\n",
      "Step 9\n",
      "Action:  [[-1.]]\n",
      "obs= [[7.015735]] reward= [1.] done= [ True]\n",
      "........x..\n",
      "Goal reached! reward= [1.]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "print('obs=', obs)\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  action, _ = model.predict(obs, deterministic=True)\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  print(\"Action: \", action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render(mode='console')\n",
    "  if done:\n",
    "    # Note that the VecEnv resets automatically\n",
    "    # when a done signal is encountered\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fb6ae",
   "metadata": {},
   "source": [
    "# Continous and \"Just Match\" Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c91a67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class GoLeftEnvContinuous2(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left. \n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnvContinuous2, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    self.action_space = spaces.Box(low=-1, high=1,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = self.observation_space.sample()[0]\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    if self.action_space.contains(action):\n",
    "      candidate_pos = self.agent_pos + action[0]\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    # self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # The agent can only step the right pace to end\n",
    "    if 0< candidate_pos < self.grid_size:\n",
    "      self.agent_pos = candidate_pos\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    done = bool(self.agent_pos == 0)\n",
    "\n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    approx_pos = int(np.ceil(self.agent_pos))\n",
    "    print(\".\" * approx_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - approx_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2164cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnvContinuous2()\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f644af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..x........\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Box(-1.0, 1.0, (1,), float32)\n",
      "[0.42100808]\n",
      "Step 1\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 2\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 3\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 4\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 5\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 6\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 7\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 8\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 9\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n",
      "Step 10\n",
      "obs= [0.75011134] reward= 0 done= False\n",
      ".x.........\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = np.array([-1])\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 10\n",
    "for step in range(n_steps):\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  obs, reward, done, info = env.step(GO_LEFT)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render()\n",
    "  if done:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4a25fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1047      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | -36.8     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000858 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.23e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1000     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | -817     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.021   |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.000632 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1016      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | -16.7     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000219 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.15e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1038     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | -2.74    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0511   |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.000503 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1055      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0.000432 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.76e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1069     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | -775     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00495  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.29e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1079      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -1.61e-05 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.58e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1091     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00296 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 6.17e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1088      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -4.45e-07 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 1.07e-13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1094      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -1.47e-05 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 2.1e-10   |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "env = GoLeftEnvContinuous2(grid_size=10)\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "# Train the agent\n",
    "model = A2C('MlpPolicy', env, verbose=1).learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a8c0392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs= [[5.741468]]\n",
      "Step 1\n",
      "Action:  [[1.]]\n",
      "obs= [[6.741468]] reward= [0.] done= [False]\n",
      ".......x...\n",
      "Step 2\n",
      "Action:  [[1.]]\n",
      "obs= [[7.741468]] reward= [0.] done= [False]\n",
      "........x..\n",
      "Step 3\n",
      "Action:  [[1.]]\n",
      "obs= [[8.741468]] reward= [0.] done= [False]\n",
      ".........x.\n",
      "Step 4\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 5\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 6\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 7\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 8\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 9\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 10\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 11\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 12\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 13\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 14\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 15\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 16\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 17\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 18\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 19\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 20\n",
      "Action:  [[1.]]\n",
      "obs= [[9.741468]] reward= [0.] done= [False]\n",
      "..........x\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "print('obs=', obs)\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  action, _ = model.predict(obs, deterministic=True)\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  print(\"Action: \", action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render(mode='console')\n",
    "  if done:\n",
    "    # Note that the VecEnv resets automatically\n",
    "    # when a done signal is encountered\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc032b4",
   "metadata": {},
   "source": [
    "# Continuous and MaxPace Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a60280c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class GoLeftEnvContinuous3(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left. \n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnvContinuous3, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    self.action_space = spaces.Box(low=-1, high=1,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    # self.agent_pos = self.grid_size - 1\n",
    "    # Random Initialize\n",
    "    self.agent_pos = self.observation_space.sample()[0]\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    if self.action_space.contains(action):\n",
    "      # we set the max pace to be -0.5\n",
    "      if action[0] < -0.5:\n",
    "            action[0] = -action[0] - 1  # if action is -0.6 it then becomes -0.4\n",
    "      self.agent_pos += action[0]\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    done = bool(self.agent_pos == 0)\n",
    "\n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    approx_pos = int(np.ceil(self.agent_pos))\n",
    "    print(\".\" * approx_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - approx_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff46a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnvContinuous3()\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09f17925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...x.......\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Box(-1.0, 1.0, (1,), float32)\n",
      "[0.147842]\n",
      "Step 1\n",
      "obs= [2.0327077] reward= 0 done= False\n",
      "...x.......\n",
      "Step 2\n",
      "obs= [1.9327075] reward= 0 done= False\n",
      "..x........\n",
      "Step 3\n",
      "obs= [1.8327076] reward= 0 done= False\n",
      "..x........\n",
      "Step 4\n",
      "obs= [1.7327076] reward= 0 done= False\n",
      "..x........\n",
      "Step 5\n",
      "obs= [1.6327076] reward= 0 done= False\n",
      "..x........\n",
      "Step 6\n",
      "obs= [1.5327076] reward= 0 done= False\n",
      "..x........\n",
      "Step 7\n",
      "obs= [1.4327075] reward= 0 done= False\n",
      "..x........\n",
      "Step 8\n",
      "obs= [1.3327076] reward= 0 done= False\n",
      "..x........\n",
      "Step 9\n",
      "obs= [1.2327076] reward= 0 done= False\n",
      "..x........\n",
      "Step 10\n",
      "obs= [1.1327076] reward= 0 done= False\n",
      "..x........\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = np.array([-0.9])\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 10\n",
    "for step in range(n_steps):\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  obs, reward, done, info = env.step(GO_LEFT)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render()\n",
    "  if done:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff6fd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "env = GoLeftEnvContinuous3(grid_size=10)\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "# Train the agent\n",
    "model = A2C('MlpPolicy', env, verbose=0).learn(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "399219a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs= [[2.3184118]]\n",
      "Step 1\n",
      "Action:  [[-0.5003018]]\n",
      "obs= [[1.8187137]] reward= [0.] done= [False]\n",
      "..x........\n",
      "Step 2\n",
      "Action:  [[-0.5039367]]\n",
      "obs= [[1.3226504]] reward= [0.] done= [False]\n",
      "..x........\n",
      "Step 3\n",
      "Action:  [[-0.5088629]]\n",
      "obs= [[0.8315133]] reward= [0.] done= [False]\n",
      ".x.........\n",
      "Step 4\n",
      "Action:  [[-0.5116877]]\n",
      "obs= [[0.34320098]] reward= [0.] done= [False]\n",
      ".x.........\n",
      "Step 5\n",
      "Action:  [[-0.49323907]]\n",
      "obs= [[0.7129183]] reward= [1.] done= [ True]\n",
      ".x.........\n",
      "Goal reached! reward= [1.]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "print('obs=', obs)\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  action, _ = model.predict(obs, deterministic=True)\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  print(\"Action: \", action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  env.render(mode='console')\n",
    "  if done:\n",
    "    # Note that the VecEnv resets automatically\n",
    "    # when a done signal is encountered\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5932ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
