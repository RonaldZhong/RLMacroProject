{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7d5105",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "50b9acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 5)  #set default figure size\n",
    "import numpy as np\n",
    "from numba import jit, float64\n",
    "from numba.experimental import jitclass\n",
    "import quantecon as qe\n",
    "from quantecon.distributions import BetaBinomial\n",
    "\n",
    "\n",
    "def get_q_w(n, a, b, w_min, w_max):\n",
    "    q = BetaBinomial(n, a, b).pdf()\n",
    "    w = np.linspace(w_min, w_max, n+1)\n",
    "    return q, w \n",
    "\n",
    "\n",
    "n, a, b, w_min, w_max = 50, 200, 100, 10, 60                    \n",
    "q_default, w_default = get_q_w(n, a, b, w_min, w_max) \n",
    "\n",
    "\n",
    "mccall_data = [\n",
    "    ('c', float64),      # unemployment compensation\n",
    "    ('beta', float64),      # discount factor\n",
    "    ('w', float64[:]),   # array of wage values, w[i] = wage at state i\n",
    "    ('q', float64[:])    # array of probabilities\n",
    "]\n",
    "\n",
    "\n",
    "@jitclass(mccall_data)\n",
    "class McCallModel:\n",
    "\n",
    "    def __init__(self, c=25, beta=0.99, w=w_default, q=q_default):\n",
    "\n",
    "        self.c, self.beta = c, beta\n",
    "        self.w, self.q = w_default, q_default\n",
    "\n",
    "    def state_action_values(self, i, v):\n",
    "        \"\"\"\n",
    "        The values of state-action pairs.\n",
    "        \"\"\"\n",
    "        # Simplify names\n",
    "        c, beta, w, q = self.c, self.beta, self.w, self.q\n",
    "        # Evaluate value for each state-action pair\n",
    "        # Consider action = accept or reject the current offer\n",
    "        accept = w[i] / (1 - beta)\n",
    "        reject = c + beta * np.sum(v * q)\n",
    "\n",
    "        return np.array([accept, reject])\n",
    "    \n",
    "@jit(nopython=True)\n",
    "def compute_reservation_wage(mcm,\n",
    "                             max_iter=500,\n",
    "                             tol=1e-6):\n",
    "\n",
    "    # Simplify names\n",
    "    c, beta, w, q = mcm.c, mcm.beta, mcm.w, mcm.q\n",
    "\n",
    "    # == First compute the value function == #\n",
    "\n",
    "    n = len(w)\n",
    "    v = w / (1 - beta)          # initial guess\n",
    "    v_next = np.empty_like(v)\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "    while i < max_iter and error > tol:\n",
    "\n",
    "        for i in range(n):\n",
    "            v_next[i] = np.max(mcm.state_action_values(i, v))\n",
    "\n",
    "        error = np.max(np.abs(v_next - v))\n",
    "        i += 1\n",
    "\n",
    "        v[:] = v_next  # copy contents into v\n",
    "\n",
    "    # == Now compute the reservation wage == #\n",
    "\n",
    "    return (1 - beta) * (c + beta * np.sum(v * q))\n",
    "\n",
    "\n",
    "mcm = McCallModel()\n",
    "reservation_wage = compute_reservation_wage(mcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21818ee8",
   "metadata": {},
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6f0e9fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "43873ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class McCallSearch(gym.Env):\n",
    "    \n",
    "    # n: num of states\n",
    "    # a, b: params of the betabinomial distribution\n",
    "    # w_min, w_max: wage lower and upper bound\n",
    "    # c: unemployment compensation\n",
    "    # beta: discount factor\n",
    "    def __init__(self, n=50, a=200, b=100, w_min=10, w_max=60, c=25, beta=0.99):\n",
    "        from quantecon.distributions import BetaBinomial\n",
    "        self.n = n\n",
    "        self.q = BetaBinomial(n, a, b).pdf()\n",
    "        self.w = np.linspace(w_min, w_max, n+1)\n",
    "        self.c = c\n",
    "        self.beta = beta\n",
    "        # the last state (n+1) is the terminial state\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=n+1, shape=(1,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        \n",
    "    def sample_space(self):\n",
    "        return np.random.choice(self.n+1, p=self.q)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([self.sample_space()]).astype(np.float32)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        wage = self.w[int(self.state[0])] if self.state[0] < self.n+1 else 0\n",
    "        # 0 denotes accepting the offer, 1 denotes rejecting\n",
    "        if action == 0:\n",
    "            done = True\n",
    "            reward = wage / (1 - self.beta)\n",
    "            self.state = np.array([self.n+1]).astype(np.float32)\n",
    "        elif action == 1:\n",
    "            reward = self.c\n",
    "            self.reset()\n",
    "        else:\n",
    "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "        return self.state, reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556810b2",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ce5a448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, a, b, w_min, w_max = 10, 200, 100, 10, 60\n",
    "c, beta = 25, 0.60\n",
    "q, w = get_q_w(n, a, b, w_min, w_max)\n",
    "\n",
    "mcm = McCallModel(c=c, beta=beta, q=q, w=w)\n",
    "reservation_wage = compute_reservation_wage(mcm)\n",
    "\n",
    "env = McCallSearch(n=n, a=a, b=b, w_min=w_min, w_max=w_max, c=c, beta=beta)\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a81f3",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6f8b7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "env = McCallSearch(n=n, a=a, b=b, w_min=w_min, w_max=w_max, c=c, beta=beta)\n",
    "# wrap it\n",
    "wrapped_env = make_vec_env(lambda: env, n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model = DQN('MlpPolicy', wrapped_env, verbose=0, gamma=env.beta).learn(200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3933d",
   "metadata": {},
   "source": [
    "# compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669be0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = McCallSearch(n=n, a=a, b=b, w_min=w_min, w_max=w_max, c=c, beta=beta)\n",
    "wages = test_env.w\n",
    "for i in range(test_env.n + 1):\n",
    "    action, _ = model.predict(np.array([i]), deterministic=True)\n",
    "    true_action = 0 if wages[i] > reservation_wage else 1\n",
    "    print(f'state: {i}, action: {action}, true action: {true_action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94f7b1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
